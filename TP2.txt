Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1:

Explicar Architecture, descrever network, input e output.
Our agent's network consist of 4 convolutional layers where each one is followed by a relu activation and a MaxPooling of 2 by 2, except the last one that is followed by a MaxPooling of 4 by 4. After the convolutional layers our model has a flatten layer to get all the features in a single dimension and 3 dense layers, where the first uses 64 neurons, the seconds 32 and the third 16, using the relu activation in all of them. These layers are followed by a last dense layer which is a dense layer with 3 neurons (1 for each action).
We chose this architecture because we started off our tests with an architecture close to one we used in the first assignment since that architecture was already improved to extract results from images. We started with an architecture that had 3 convolution layers and 3 dense layers, and after some tests, reducing and increasing the number of layers, we reached the conclusion that an architecture with 4 convolutional layers and 4 dense layers was the one giving us the best results. This can be seen in the images apples_layers and steps_layers where we trained 3 different models, one with 2 convolutional layers and 2 dense layers, one with 3 convolutional layers and 3 dense layers and another one with 4 convolutional layers and 4 dense layers for closely to 45 minutes each. After training them, we tested them with a limmit of 500 steps for each game to prevent the games of being stuck in an infinity loop in the worse models. As can be seen in the images apple_layers and steps_layers, the model with 4 convolutional layers and 4 dense layers is the one that got less stuck in infinity loops (had a smaller average number of steps) and got the highest score per game on average. After obtaining these results we chose the model with 4 convolutional layers and 4 dense layers. After this we tried reducing the number of poolings, but didn't get any better results by doing less poolings than we had (1 after each convolutional layer). We also tried using dropout on the dense layers, but that only gave us worse results.
Input: The input to our agent is an observation of the board state .
Output: Best action that the agent thinks it is possible in the current board state. Model of the architecture explained above that will be used to predict the expected reward for each action.

Depois: 
De acordo com as experiências do primeiro trabalho testámos modelos à volta de X que foram melhorados para extrair resultados de imagens no primeiro trabalho e alterámos alguns parâmetros como o número de camadas e de neurónios por camada de modo a ver se algum destes parâmetros ia ter influência neste novo problema. Nas imagens Z1, Z2, Z3, podemos ver que diminuir o número de camadas convolucionais faz X, manter as do primeiro trabalho faz Y e aumentar faz H. Testámos também diferentes Poolings para o número de camadas que nos pareceu dar melhores resultados, testando Poolings sempre de 2,2 como visto na imagem H, sempre 3,3 como visto na imagem Y, ir aumentando como visto na imagem Z e ir diminuindo como visto na imagem K. Testámos também realizar menos poolings como pode ser visto na imagem K deu-nos piores resultados. Além de alterar o número de camadas convolucionais, testámos também aumentar e diminuir o número de camadas densas, como pode ser visto nas imagens X, Y e Z. Experimentámos ainda alterar a loss function para o mean squared error, mas como observado na imagem H os resultados só pioraram. e initialization RandomNormal(stddev=0.01)

Plots - numero average de apples por jogo e numero de passos, número de passos por maçã
Diferentes camadas Conv2D + Diferentes camadas densas 3 models // Diferentes Poolings cagar neste// Diferente número de Poolings //  // Diferente loss // Diferente initialiation
Nestes plots mostramos o número de sobrevivência em vez do número de maçãs porque iriam demorar demasiado tempo a ter plots relevantes com o número de maçãs.

Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2:

The scenario for which we tried to optimize our agent was for a snake game with a board of 30 by 30 plus border (which would leave us with a 32 by 32 game) with a single apple and no grass. We chose this scenario because we tested to add grass with different growth rates and different rewards per grass eaten by the snake, but this did not seem to improve our model, in some cases where we tested with a high reward on grass our snake got stuck in an infinity loop eating only grass in the same loop and not caring about the apple. We also tested training our model with more apples per game, but our heuristic ignores the other apple, it only consider the first one, and after some tests and watching the images of our snake playing, she seemed to ignore most of the apples during train, and did not seem to improve the score after training. Besides testing with grass and more apples, we also tested with smaller boards. The results we obtained in these scenarios didn't seem to improve the score and didn't help on the number of steps the snake survived. 

Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3:

exploration feita no treino

Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4:

The algorithm that we implemented to train the agent was the Q-Learning algorithm since in this specific problem we want to choose an action that will have in consideration the maximum value (maximum reward) of the next step, this meaning, that will leave us closer to the apple or atleast not dying. 
The discount factor (gamma) picked was - To choose the best discount factor we started by trying 0.1, 0.5, 0.8 and 0.9 . The problem with 0.1 was that the model got allways stuck in loops without getting any apple and this could be because he's giving really low importance to future rewards compared to the current next one, and this way the model ends up trying not dying in the next step more than getting an apple really far away in the future. This problem seemed to get better when we tested a discount factor of 0.5, where he got less stuck in loops and started getting more apples in average. With 0.8 and 0.9 as discount factors, the model gave to much importance to the future and didn't get as many apples in average, this could be because the model doesn't mind taking longer to get an apple since it's discounting less on the reward. We can see this experiment in the images apples_gamma and steps_gamma where we trained these 4 different models for about 20-30 minutes each and got the average apples per game and average steps per game, finishing the game at a maximum of 500 steps, to prevent infinity loops. As can be seen in the image, the model with 0.1 gamma always got stuck in loops without getting any apples, the models with 0.8 and 0.9 also got stuck in loops but eventually got some apples, since they don't mind getting an apple further in the future and the model that seems having the best compromise is the model with 0.5 gamma. 
The schedule for generating new experiences and training the agent chosen was generating 50000 experiences with our heuristic before starting the train and every 10000 actions while training we generate 2000 more new experiences using our heuristic. We tried different alternatives like generating also 5000 and 10000 random experiences, after the 50000 generated with our heuristic, before training so our agent would also have some bad experiences to learn what not to do. But that did not seem to improve the predictions, it actually made them worse so we decided to keep only the experiences created by the heuristic before starting the train. We also tried having experiences being generated with our heuristic more often during training (instead of every 10000, it was every 5000) but that did not seem to improve the results. 
We decided to train our agent every 16 steps. After trying every 4, 8, 16, 64, 100, we decided that 16 would be a good compromise between number of actions to make and training again since with less we would have litle change to the examples given to the agent and wouldn't be worth training him, and would also take longer to train the same number of examples. Training with less frequency would train faster the same number of examples but wouldn't give us such good results as training every 16 steps.

Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5:

Íamos escolher arquiteturas de redes e parâmetros com base em treinos maiores. Realizar mais testes onde aumentássemos mais a relva com diferentes growth_rates.
More tests with a bigger board and more apples 


